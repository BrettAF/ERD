# -*- coding: utf-8 -*-
"""Windowing and training

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KgzwsS16sqLLQ-XMEMecHoObOc6HgEEJ

# Libraries and variables
"""

window_size = 600 #This is the length of each frame
number_of_channels=18 #The number of channels data is recieved from

percent_testing=.04 #The percentage of the data that will be used for testing
percent_validation=.04 #The percentage of the data that will be used for validation
percent_training=.4 #The percentage of the data that will be used for training

batch_size = 128
number_of_epochs=300


#This is the period before a seizure that will be examined.
Period_of_interest=60*60*1

folders_for_making_model = ('chb01','chb03','chb05',)

import os
import pandas as pd
import numpy as np

from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.python import training

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)
import random as r
import scipy as sp
from scipy.stats import zscore
from scipy.fft import fft, ifft
import copy

from keras.layers.rnn import LSTM

#This array holds all the EEG datasets this program will be using
dfs=[]
for folder in folders_for_making_model:
  print(folder)
  os.chdir(folder)
  for x in os.listdir():
    if x.endswith("formated.csv"):
      print(x)
      # This appends all the csv files to the array
      dfs.append( pd.read_csv(str(x)))
  #Go back so that a new folder can be found
  os.chdir("..")

#These lines of code take the variables from the beginning, calculate how many windows will be needed, and prints some information


time_between_signals = 3.906266
#This window size generates a number of samplings in a window of specific length
number_of_windows=(len(dfs)*625000/600)
window_time= (window_size*time_between_signals)/1000

validation_windows=int(number_of_windows*percent_validation)
testing_windows=   int(number_of_windows*percent_testing)
training_windows=  int(number_of_windows*percent_training)

print("the Period of interest is",Period_of_interest/3600,"hour\n",
      "This setup will generate:\n",training_windows , "windows for training,",percent_training,"\n",
      validation_windows,"windows for validation &",percent_validation,"\n",
      testing_windows,"Windows for testing,",percent_testing,".\n",
      "Each window has",window_time,"seconds, and",window_size,"timesteps\n",
      "This is about",number_of_windows*window_time/60/60,"hours of data\n")

"""# Reading & Shaping Data"""

#for recording the lables of the dataset as a one-hot vector
def LabelVector3(label):
  zero_vector=[1,0,0]
  one_vector =[0,1,0]
  two_vector =[0,0,1]

  if label==0:
      return zero_vector
  elif label ==1:
      return one_vector
  else: return two_vector

#This function is the same as the last, but with the seizures excluded
def LabelVector2(label):
  zero_vector=[1,0]
  one_vector =[0,1]
  if label==0:
      return zero_vector
  else :
      return one_vector

#This ensures that no two training windows are the same.
#It generates a vector of shuffled unique start locations
#It creates two vectors, one that indicates what array to draw data from,
#and one that indicates at what row in that dataset to start at
def vectorMakerV2():
  arr=[]
  for df in range(0,len(dfs)):
    for row in range(0,len(dfs[df])-window_size-1,window_size):
        arr.append([df,row])
  #This shuffles the starting locations without seperating the dataset from the start time.
  np.random.shuffle(arr)
  arr=np.array(arr)

  window_df_starts = list(arr[:,0])
  window_starts= list(arr[:,1])
  return window_starts,window_df_starts

def frameArrayMakerV3(i, window_length,  window_starts, window_df_starts,normalize,fft_flag, include_seizures):
  frames_array=[]
  label_array=[]
  label_OHV=[]

  while i<window_length:
    fft_array=[]
    start=window_starts[i]
    df=window_df_starts[i]
    i=i+1
    #Still ordered data from a dataframe of a specific size
    new_df=(dfs[df]).loc[start:start+window_size-1]

  #Stores the labels in an array
    #If the number is negative, it is a seizure and is marked with a 2
    if (new_df.at[start+window_size-1,"Label"] <0):
      new_label=2
    #if the label is 0, it is after the last siezure, i don;t really know how to treat it
    elif (new_df.at[start+window_size-1,"Label"] ==0):
      new_label = 0
    #if the label is greater than the poi, then it is far from the seizure and is marked with a 0
    elif(new_df.at[start+window_size-1,"Label"] >Period_of_interest):
      new_label=0
    #If none of the others were true, it must be in the period of interest, and it gets marked with a 1
    else:
      new_label=1


    if ((include_seizures == True) or (new_label != 2) ):
      new_df=new_df.drop(columns=["EEG Time", "Label"])
      # normalizes the columns
      if normalize == True:
          new_df = zscore(new_df)
      label_array.append(new_label)

      if (include_seizures == True):
          label_OHV.append( LabelVector3(new_label))
      else:
          label_OHV.append( LabelVector2(new_label))
      frames_array.append( new_df)

  return frames_array, label_array, label_OHV,i

#This function guarantees that the number of each training label should stay aproximately even
#it moves data from the data 'tail' that is not placed inside the datasets into the training set to
# move it closer to the correct ratio
#it may not achieve the 49.5% ratio, but it will return anyway
def rebalanceRatio(training_ds_p, training_lables_p,training_labels_OHV_p):

  # These make the actual training windows
  training_ds=training_ds_p[0:training_windows]
  training_lables=training_lables_p[0:training_windows]
  training_labels_OHV=training_labels_OHV_p[0:training_windows]

  ratio=training_lables.count(1)/training_windows
  print("ratio before adjusting:",ratio)

  #a moves through the tail
  #b moves through the training window

  a=copy.copy(training_windows)
  b=0
  while ratio >.505 and(a<len(training_ds_p)-2):
    while training_lables_p[a] !=0and (a<len(training_ds_p)-1):
      a+=1
    while training_lables[b] !=1:
      b+=r.randint(1,4)
    training_lables[b]=training_lables_p[a]
    training_ds[b]=training_ds_p[a]
    training_labels_OHV[b]=training_labels_OHV_p[a]
    a+=1
    if b>training_windows:
      b=0
    b+=r.randint(1,3)
    ratio=training_lables.count(1)/training_windows

  while ratio <.495 and(a<len(training_ds_p)-1):
    while training_lables_p[a] !=1 and (a<len(training_ds_p)-1):
      a+=1
    while training_lables[b] !=0 and (b<training_windows-1):
      b+=r.randint(1,4)
    training_lables[b]=training_lables_p[a]
    training_ds[b]=training_ds_p[a]
    training_labels_OHV[b]=training_labels_OHV_p[a]
    a+=1
    if b>training_windows:
      b=0
    b+=r.randint(1,3)
    ratio=training_lables.count(1)/training_windows

  return training_ds,training_lables,training_labels_OHV

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
#If print ratio is true, it will print the number of each type of label and their ratio
#if normalize is true, it will perform z-score normalization
def get_dataset_partitions_tf(dfs, hot_vector=True, print_ratio = True, normalize=False, fft=False, include_seizures = True):
    print("fft:",fft,', normalize:',normalize,", include Seizures:",include_seizures)
    window_starts,window_df_starts= vectorMakerV2()
    print('.',end='')
    #For testing data
    testing_ds, testing_lables,testing_labels_OHV,i =frameArrayMakerV3(0,testing_windows+1,  window_starts,window_df_starts,normalize,fft,include_seizures)

    #For validation data
    validation_ds, validation_lables,validatiion_labels_OHV,i =frameArrayMakerV3(i,validation_windows+testing_windows+1,  window_starts,window_df_starts,normalize,fft,include_seizures)
    print('.',end='')
     #for training data
    training_ds_p, training_lables_p,training_labels_OHV_p,i =frameArrayMakerV3(i,len(window_starts), window_starts,window_df_starts,normalize,fft,include_seizures)
    #print(i,window_starts[30],window_df_starts[30])
    print('\n---Training Data---')
    print('.',end='')
    training_ds, training_lables,training_labels_OHV = rebalanceRatio(training_ds_p, training_lables_p,training_labels_OHV_p)
    #0: not poi or seizure
    #1: period of interest
    #2: seizure



    if print_ratio == True:
      print("# of POI     :",training_lables.count(1),",",100*training_lables.count(1)/training_windows,'%')
      print("# of not POI :",training_lables.count(0),",",100*training_lables.count(0)/training_windows,'%')
      print("# of seizures:",training_lables.count(2),",",100*training_lables.count(2)/training_windows,'%','\n')
    if hot_vector == True:
      return training_ds, testing_ds, validation_ds, training_labels_OHV, testing_labels_OHV,validatiion_labels_OHV
    return training_ds, testing_ds, validation_ds, training_lables, testing_lables, validation_lables

from google.colab import drive
drive.mount('/content/drive')

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
train_ds, test_ds, valid_ds, train_lables, test_lables,  valid_lables = get_dataset_partitions_tf(dfs,print_ratio=True, normalize=True, include_seizures=False, hot_vector=True)

training_data = np.array(train_ds)
testing_data = np.array(test_ds)
validation_data= np.array(valid_ds)


training_lables= np.array(train_lables)
testing_lables= np.array(test_lables)
validation_lables=np.array(valid_lables)


print("training  ",training_data.shape,",",training_lables.shape)
print("testing   ",testing_data.shape,",",testing_lables.shape)
print("validating",validation_data.shape,",",validation_lables.shape)
print('\n')

"""# Models & Training"""

learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=.01,
                                                                first_decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)
opt = keras.optimizers.legacy.Adam(learning_rate=learning_rate)

# normalized .5466,
input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(-1,window_size, number_of_channels, 1)

print("training  ",training_data.shape,",",training_lables.shape)
m2Dlstm = keras.Sequential([
  layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
  layers.Conv2D(filters=16, kernel_size=(3,3)),
  layers.MaxPool2D( pool_size=(11, 5)),
  layers.Reshape((54,32)),
  layers.LSTM(90, activation='relu' ),
  layers.Dense(75),
  layers.Dense(40),
  layers.Dense(training_lables.shape[1])
])
print('\n',m2Dlstm.summary())
m2Dlstm.compile(
              optimizer='Adam',
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])

m2Dlstm.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

"""# m1DLSTM

"""

input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(training_windows,-1, number_of_channels,1)

m1DLSTM = keras.Sequential([
  layers.Conv1D(filters=64, kernel_size=3, input_shape=input_shape),
  layers.Conv1D(filters=64, kernel_size=3),
  layers.MaxPool2D(),
  layers.Reshape([300,448]),
  layers.LSTM(100, activation='relu' ),
  layers.Dense(100),
  layers.Dense(50),
  layers.Dense(2)

])
print(m1DLSTM.summary())
m1DLSTM.compile(
              optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
m1DLSTM.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(training_windows,window_size, number_of_channels, 1)

m2Dconv = keras.Sequential([
  layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
  layers.Conv2D(filters=16, kernel_size=(3,3)),
  layers.MaxPool2D( pool_size=(50, 5)),
  layers.Flatten(),
  layers.Dense(100),
  layers.Dense(training_lables.shape[1])
])
print(m2Dconv.summary())
m2Dconv.compile(
              optimizer='Adam',
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])
#normalized:.4955 , non-normalized: 0.4955
m2Dconv.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

print('\n--Accuracy on testing data--')
print('m2Dlstm')
test_loss, test_acc = m2Dlstm.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)

print('m1DLSTM')
test_loss, test_acc = m1DLSTM.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)

print('m2Dconv')
test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)