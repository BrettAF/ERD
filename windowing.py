# -*- coding: utf-8 -*-
"""Windowing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wnBVijSD_Qy4XOfZKya4JuVc6t2SQSwR
"""



"""# Libraries and variables"""

import os
import pandas as pd
import numpy as np
import copy

from pathlib import Path


# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)
import random as r
import scipy as sp
from scipy.stats import zscore
from scipy.fft import fft, ifft

from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from numpy import where

from keras.layers.rnn import LSTM

window_size = 600 #This is the length of each frame
number_of_channels=18 #The number of channels data is recieved from

percent_testing=.05 #The percentage of the data that will be used for testing
percent_validation=.05 #The percentage of the data that will be used for validation
percent_training=.83 #The percentage of the data that will be used for training


#This is the period before a seizure that will be examined.
Period_of_interest=60*60*1

folders_for_making_model = ('chb01',)
#,'chb02','chb03','chb04','chb05'

"""# Reading & Shaping Data"""

#This ensures that no two training windows are the same.
#It generates a vector of shuffled unique start locations
#It creates two vectors, one that indicates what array to draw data from,
#and one that indicates at what row in that dataset to start at
def vectorMakerV2():
  arr=[]
  for df in range(0,len(dfs)):
    for row in range(0,len(dfs[df])-window_size-1,window_size):
        arr.append([df,row])
  #This shuffles the starting locations without seperating the dataset from the start time.
  np.random.shuffle(arr)
  arr=np.array(arr)

  window_df_starts = list(arr[:,0])
  window_starts= list(arr[:,1])
  return window_starts,window_df_starts


dfs=[] #This array holds all the EEG datasets this program will be using
for folder in folders_for_making_model:
  print(folder)
  os.chdir(folder)
  for x in os.listdir():
    if x.endswith("formated.csv"):
      print(x)
      # This appends all the csv files to the array
      dfs.append( pd.read_csv(str(x)))
  #Go back so that a new folder can be found
  os.chdir('..')

window_starts,window_df_starts= vectorMakerV2()

#These lines of code take the variables from the beginning, calculate how many windows will be needed, and prints some information


time_between_signals = 3.906266
#This window size generates a number of samplings in a window of specific length
number_of_windows=(len(window_starts))
window_time= (window_size*time_between_signals)/1000
validation_windows=int(number_of_windows*percent_validation)
testing_windows=   int(number_of_windows*percent_testing)
training_windows=  int(number_of_windows*percent_training)

print("the Period of interest is",Period_of_interest/3600,"hour\n",
      "there are",number_of_windows,'windows,\n'
      " This setup will generate:\n",training_windows , "windows for training,",percent_training,"%\n",
      validation_windows," windows for validation",percent_validation,"% &\n",
      testing_windows," Windows for testing,",percent_testing,"%.\n",
      "Each window has",window_time,"seconds, and",window_size,"timesteps\n",
      "This is about",number_of_windows*window_time/60/60,"hours of data\n")

def frameArrayMakerV4(i,desired_length, normalize,fft, include_seizures,filename):
  frames_array=np.memmap(filename+"_df", dtype='float32', mode='w+', shape=(desired_length,window_size,number_of_channels))
  label_array=[]
  j=0
  #i is the place we are searching
  #j is the place we are placing
  while len(label_array)<desired_length-1:
    fft_array=[]
    start=window_starts[i]
    df=window_df_starts[i]
    #Still ordered data from a dataframe of a specific size
    new_df=(dfs[df]).loc[start:start+window_size-1]

    #Stores the labels in an array
    #If the number is negative, it is a seizure and is marked with a 2
    if (new_df.at[start+window_size-1,"Label"] <0):
      new_label=2
    #if the label is 0, it is after the last siezure, i don;t really know how to treat it
    elif (new_df.at[start+window_size-1,"Label"] ==0):
      new_label = 0
    #if the label is greater than the poi, then it is far from the seizure and is marked with a 0
    elif(new_df.at[start+window_size-1,"Label"] >Period_of_interest):
      new_label=0
    #If none of the others were true, it must be in the period of interest, and it gets marked with a 1
    else:
      new_label=1


    if ((include_seizures == True) or (new_label != 2) ):
      new_df=new_df.drop(columns=["EEG Time", "Label"])
      # normalizes the columns
      if normalize == True:
          new_df = zscore(new_df)
      label_array.append(new_label)
      frames_array[j]= new_df
      j=j+1
      frames_array.flush()
    i=i+1
  np.array(label_array)
  np.save(filename+"_lables",label_array)

  return i

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
#If print ratio is true, it will print the number of each type of label and their ratio
#if normalize is true, it will perform z-score normalization
def get_dataset_partitions_tf(dfs, normalize=False, fft=False, include_seizures = True):
    print("fft:",fft,', normalize:',normalize,", include Seizures:",include_seizures)
    i=0
    #For testing data
    i=frameArrayMakerV4(i,testing_windows ,normalize,fft,include_seizures,'testing')
    print(i,end=' ')

    #For validation data
    i=frameArrayMakerV4(i,validation_windows,normalize,fft,include_seizures,'validation')
    print(i,end=' ')

    #for training data
    i =frameArrayMakerV4(i,training_windows,normalize,fft,include_seizures,'training')
    print(i,end=' ')

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
get_dataset_partitions_tf(dfs, normalize=True, include_seizures=False)

training_lables=np.load("training_lables.npy")
training_data= np.memmap('training_df', mode='w+',dtype='float32',shape=(training_windows,window_size*number_of_channels))
print("training  ",training_data.shape,",",training_lables.shape)



# summarize class distribution
counter = Counter(training_lables)

# define pipeline
over = SMOTE(sampling_strategy=1)
under = RandomUnderSampler(sampling_strategy=1)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)


# transform the dataset
training_data, training_lables = pipeline.fit_resample(training_data, training_lables)
# summarize the new class distribution
print("before:\t",counter)
counter = Counter(training_lables)
print("after:\t",counter)
print("training  ",training_data.shape,",",training_lables.shape)

training_data=training_data.reshape(-1,window_size,number_of_channels)
np.save('training_data',training_data)
np.save("training_lables",training_lables)

os.remove('training_df')
training_data=0
training_lables=0

testing_df=np.memmap('testing_df', dtype='float32',shape=(testing_windows,window_size*number_of_channels) )
np.save('testing_data',testing_df)
testing_df = 0
os.remove('testing_df')

validation_df=np.memmap("validation_df", dtype='float32',shape=(validation_windows,window_size*number_of_channels))
np.save("validation_data",validation_df)
validation_df = 0
os.remove(validation_df)