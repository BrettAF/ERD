# -*- coding: utf-8 -*-
"""ModelsV3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbh0sgFnhcrcW02DCMiE4j-JAQ0ekBrt
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.python import training
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import numpy as np
import time
import csv
import os

batch_size=200
number_of_epochs=15
window_size = 600
number_of_channels = 18

def make_dataset(subjects):
  data_files=[]
  data_labels=[]
  for subject in subjects:
    labels=np.load(subject+'data_labels.npy')
    files=np.load(subject+'data_files.npy')
    data_files=np.append(data_files,files)
    data_labels=np.append(data_labels,labels)
  data_labels=oneHot(data_labels)
  print(data_labels.shape)
  data_files,vailidation_data,data_labels,vailidation_labels=train_test_split(data_files,data_labels, test_size=0.10, random_state=42)
  training_data,testing_data,training_labels,testing_labels=train_test_split(data_files,data_labels, test_size=0.10, random_state=42)

  return training_data,training_labels,testing_data,testing_labels,vailidation_data,vailidation_labels

"""
# Models"""

def fun2dlstm(my_training_generator,window_size,number_of_channels,my_validation_generator):
  number_of_channels=18

  outputs=2
  input_shape = ( window_size, number_of_channels, 1)


  m2Dlstm = keras.Sequential([
      layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
      layers.Conv2D(filters=16, kernel_size=(3,3)),
      layers.MaxPool2D( pool_size=(11, 5)),
      layers.Reshape((54,32)),
      layers.LSTM(90, activation='relu' ),
      layers.Dense(75),
      layers.Dense(40),
      layers.Dense(outputs)
  ])
  print('\n',m2Dlstm.summary())
  m2Dlstm.compile(
                  optimizer='Adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(),
                  metrics=['accuracy'])

  m2Dlstm.fit(my_training_generator,
                   workers=15,
                   steps_per_epoch = int(2000 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1, validation_data=my_validation_generator)
  m2Dlstm.save("m2dlstm")
  return m2dlstm

"""history = model.fit(
                    train_generator,
                    steps_per_epoch=train_steps,
                    epochs=train_epochs,
                    validation_data=validation_generator,
                    validation_steps=validation_steps,
                    class_weight=class_weights,
                    initial_epoch=init_epoch_train,
                    max_queue_size=15,
                    workers=8,
                    callbacks=callbacks_list
                    )
"""

def fun1DLSTM(my_training_generator,window_size,number_of_channels,my_validation_generator):
  input_shape = ( window_size, number_of_channels, 1)
  outputs=2

  m1DLSTM = keras.Sequential([
    layers.Conv1D(filters=64, kernel_size=3, input_shape=input_shape),
    layers.Conv1D(filters=64, kernel_size=3),
    layers.MaxPool2D(),
    layers.Reshape([300,448]),
    layers.LSTM(100, activation='relu' ),
    layers.Dense(100),
    layers.Dense(50),
    layers.Dense(outputs)

  ])
  print(m1DLSTM.summary())
  m1DLSTM.compile(
                optimizer='Adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])

  m1DLSTM.fit_generator(generator=my_training_batch_generator,
                   steps_per_epoch = int(3800 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1, validation_data=my_validation_generator)
  m1DLSTM.save('m1DLSTM')
  return m1DLSTM

def fun2Dconv(my_training_generator,window_size,number_of_channels,my_validation_generator):
  outputs=2

  input_shape = ( window_size, number_of_channels, 1)


  m2Dconv = keras.Sequential([
    layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
    layers.Conv2D(filters=16, kernel_size=(3,3)),
    layers.MaxPool2D( pool_size=(50, 5)),
    layers.Flatten(),
    layers.Dense(100),
    layers.Dense(outputs)
  ])
  print(m2Dconv.summary())
  m2Dconv.compile(
                optimizer='Adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])


  m2Dconv.fit_generator(generator=my_training_batch_generator,
                   steps_per_epoch = int(3800 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1, validation_data=my_validation_generator)
  m2Dconv.save("m2DLSTM")
  return m2Dconv

def print_report(my_testing_generator,m2dlstm,m1DLSTM,m2Dconv):
  curr_time = str(time.strftime("%m-%d_%H:%M", time.localtime()))

  with open('results_'+curr_time+'.txt',"w") as writer:
    writer.write("batch size:"+str(batch_size)+"\tnumber of epochs:"+str(number_of_epochs))
    print('\n--Accuracy on testing data--')

    print('m2Dlstm')
    test_loss, test_acc = m2Dlstm.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write(str('m2Dlstm:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc)))

    print('m1DLSTM')
    test_loss, test_acc = m1DLSTM.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write('m1DLSTM:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc))

    print('m2Dconv')
    test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write('m2Dconv:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc))
  writer.close

"""# Generator"""

learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=.01,
                                                                first_decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)
opt = keras.optimizers.legacy.Adam(learning_rate=learning_rate)

class My_Custom_Generator(keras.utils.Sequence) :

  def __init__(self, filenames, labels, batch_size) :
    self.filenames = filenames
    self.labels = labels
    self.batch_size = batch_size


  def __len__(self) :
    return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)


  def __getitem__(self, idx) :
    batch_x = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]
    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]
    print(idx,end=' ')
    return np.array([np.memmap(str(file_name), dtype='float32', mode='r', shape=(600,18))
               for file_name in batch_x]), np.array(batch_y)

#This function converts integers to one-hot arrays
def oneHot(labels):
  zero_vector=[1,0]
  one_vector =[0,1]
  new_array=[]
  for i in labels:
    if i==0:
        new_array.append(zero_vector)
    else :
        new_array.append(one_vector)
  new_array=np.array(new_array)
  return new_array

"""# MAIN"""

np_files=['chb01','chb02','chb03','chb04','chb05','chb06','chb07','chb08','chb09','chb10','chb11','chb13','chb14','chb15','chb16','chb17','chb18','chb19','chb20', 'chb21','chb22','chb23','chb23']
l=250

training_data,training_labels,testing_data,testing_labels,vailidation_data,vailidation_labels= make_dataset(np_files)

my_training_generator = My_Custom_Generator(training_data, training_labels, batch_size)
my_validation_generator = My_Custom_Generator(vailidation_data,vailidation_labels,batch_size)
my_testing_generator = My_Custom_Generator(testing_data, testing_labels, batch_size)

m2dlstm=fun2dlstm(my_training_generator,window_size,number_of_channels,my_validation_generator)
m1DLSTM=fun1DLSTM(my_training_generator,window_size,number_of_channels,my_validation_generator)
m2Dconv=fun2Dconv(my_training_generator,window_size,number_of_channels,my_validation_generator)
print_report(my_testing_generator,m2dlstm,m1DLSTM,m2Dconv)

test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_labels, verbose=2)
print( test_acc)
