# -*- coding: utf-8 -*-
"""ModelsV5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lojs9Du_pSefrFAjFEWHq8cECUDlVl7a
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.models import Model
from keras.layers import Conv2D, Flatten, Dense, Dropout, Input, MaxPool3D, GRU, Reshape, TimeDistributed, LSTM,GlobalMaxPool2D, MaxPool2D, BatchNormalization
from keras.regularizers import l1

from tensorflow.python import training
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from collections import Counter
import numpy as np
import time
import csv
import os

batch_size=200
number_of_epochs=30
window_size = 600
number_of_channels = 18

def make_dataset(subjects):
  data_files=[]
  data_labels=[]
  for subject in subjects:
    labels=np.load(subject+'data_labels.npy')
    print(subject,':',labels.shape)
    files=np.load(subject+'data_files.npy')
    data_files=np.append(data_files,files)
    data_labels=np.append(data_labels,labels)
  data_labels=oneHot(data_labels)
  print(data_labels.shape)
  data_files,vailidation_data,data_labels,vailidation_labels=train_test_split(data_files,data_labels, test_size=0.10, random_state=42)
  training_data,testing_data,training_labels,testing_labels=train_test_split(data_files,data_labels, test_size=0.10, random_state=42)

  return training_data,training_labels,testing_data,testing_labels,vailidation_data,vailidation_labels

"""
# Models"""

def funLongConvLSTM(my_training_generator,window_size,number_of_channels,my_validation_generator,my_testing_generator):
  number_of_channels=18

  outputs=2
  input_shape = ( window_size, number_of_channels, 1)

  longConvLSTM = keras.Sequential([
      layers.Conv2D(filters=32, kernel_size=(3,1) ,input_shape=input_shape),
      layers.Conv2D(filters=16, kernel_size=(5,1)),
      layers.MaxPool2D( pool_size=(42, 2,)),
      layers.Reshape((14,144)),

      layers.LSTM(73, activation='relu' ),
      layers.Dense(73),
      layers.Dense(40),
      layers.Dense(outputs,activation='softmax')
  ])
  longConvLSTM.compile(
                  optimizer='Adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(),
                  metrics=['accuracy'])

  print('\n',longConvLSTM.summary())

  longConvLSTM.fit(my_training_generator,
                   epochs = number_of_epochs,
                   verbose = 1, validation_data=my_validation_generator)
  longConvLSTM.save("DenseLSTM")
  test_loss, test_acc = longConvLSTM.evaluate(my_testing_generator, verbose=2)

  results_string=(str( str(longConvLSTM)+':\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc)+'\n' ))
  print( results_string)
  return results_string,longConvLSTM

def funLongConvDense(my_training_generator,window_size,number_of_channels,my_validation_generator,my_testing_generator):
  number_of_channels=18

  outputs=2
  input_shape = ( window_size, number_of_channels, 1)

  longConvLSTM = keras.Sequential([
      layers.Conv2D(filters=32, kernel_size=(3,1) ,input_shape=input_shape),
      layers.Conv2D(filters=16, kernel_size=(5,1)),
      layers.MaxPool2D( pool_size=(6, 1)),

      layers.Conv2D(filters=8, kernel_size=(3,1)),
      layers.Conv2D(filters=8, kernel_size=(5,1)),
      layers.MaxPool2D( pool_size=(18, 1)),

      layers.Flatten(),
      layers.Dense(720),
      layers.Dense(720),
      layers.Dense(100),
      layers.Dense(outputs,activation='softmax')
  ])
  longConvLSTM.compile(
                  optimizer='Adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(),
                  metrics=['accuracy'])

  print('\n',longConvLSTM.summary())

  longConvLSTM.fit(my_training_generator,
                   epochs = number_of_epochs,
                   verbose = 1, validation_data=my_validation_generator)
  longConvLSTM.save("DenseLSTM")
  test_loss, test_acc = longConvLSTM.evaluate(my_testing_generator, verbose=2)

  results_string=(str( str(longConvLSTM)+':\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc)+'\n' ))
  print( results_string)
  return results_string,longConvLSTM

def fun2Dconv(my_training_generator,window_size,number_of_channels,my_validation_generator):
  outputs=2

  input_shape = ( window_size, number_of_channels, 1)


  m2Dconv = keras.Sequential([
    layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
    layers.Conv2D(filters=16, kernel_size=(3,3)),
    layers.MaxPool2D( pool_size=(50, 5)),
    layers.Flatten(),
    layers.Dense(100),
    layers.Dense(outputs, activation='softmax')
  ])
  print(m2Dconv.summary())
  m2Dconv.compile(
                optimizer='Adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])


  m2Dconv.fit(my_training_generator,
              epochs = number_of_epochs,
              verbose = 1,
              validation_data=my_validation_generator)
  m2Dconv.save("m2DLSTM")
  return m2Dconv

"""# Generator"""

learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=.01,
                                                                first_decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)
opt = keras.optimizers.legacy.Adam(learning_rate=learning_rate)

class My_Custom_Generator(keras.utils.Sequence) :

  def __init__(self, filenames, labels, batch_size) :
    self.filenames = filenames
    self.labels = labels
    self.batch_size = batch_size

  #This defines the number of batches needed to view the entire dataset
  def __len__(self) :
    return int(len(self.filenames) // float(self.batch_size))

  #This returns a tupple with the dataset
  def __getitem__(self, idx) :
    os.chdir('/content/drive/MyDrive/ERD/files')
    batch_x = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]
    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]
    return np.array([np.memmap(str(file_name), dtype='float32', mode='r', shape=(600,18))
               for file_name in batch_x]), np.array(batch_y)

#This function converts integers to one-hot arrays
def oneHot(labels):
  zero_vector=[1,0]
  one_vector =[0,1]
  new_array=[]
  for i in labels:
    if i==0:
        new_array.append(zero_vector)
    else :
        new_array.append(one_vector)
  new_array=np.array(new_array)
  counter = Counter(labels)
  print(counter)
  return new_array

def print_report(my_testing_generator,models):
  curr_time = str(time.strftime("%m-%d_%H:%M", time.localtime()))
  os.chdir('..')
  with open('results_'+curr_time+'.txt',"w") as writer:

    writer.write("batch size:"+str(batch_size)+"\tnumber of epochs:"+str(number_of_epochs))
    print('\n--Accuracy on testing data--')
    for model in models:
      print(str(model))
      test_loss, test_acc = model.evaluate(my_testing_generator, verbose=2)
      print( test_acc)
      writer.write(str( str(model)+':\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc)+'\n' ))


  writer.close

"""# MAIN"""

np_files=['chb01',]#'chb02','chb03','chb04','chb05','chb06','chb07','chb08','chb09','chb10','chb11','chb13','chb14','chb15','chb16','chb17','chb18','chb19','chb20', 'chb21','chb22','chb23','chb23']
l=250
os.chdir('/content/drive/MyDrive/ERD')

training_data,training_labels,testing_data,testing_labels,vailidation_data,vailidation_labels= make_dataset(np_files)
os.chdir('files')

my_training_generator = My_Custom_Generator(training_data, training_labels, batch_size)
my_validation_generator = My_Custom_Generator(vailidation_data,vailidation_labels,batch_size)
my_testing_generator = My_Custom_Generator(testing_data, testing_labels, batch_size)
arguments=(my_training_generator,window_size,number_of_channels,my_validation_generator,my_testing_generator)

results_string,longConvDense=funLongConvDense(my_training_generator,window_size,number_of_channels,my_validation_generator,my_testing_generator)
#denseLSTM=funDenseLSTM(arguments)

#m1DLSTM=fun1DLSTM(my_training_generator,window_size,number_of_channels,my_validation_generator)
#m2Dconv=fun2Dconv(my_training_generator,window_size,number_of_channels,my_validation_generator)
#funDenseLSTM(my_training_generator,window_size,number_of_channels,my_validation_generator)

#print_report(my_testing_generator,(longConvLSTM,denseLSTM))

#test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_labels, verbose=2)
#print( test_acc)