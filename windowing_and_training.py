# -*- coding: utf-8 -*-
"""Windowing and training

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KgzwsS16sqLLQ-XMEMecHoObOc6HgEEJ

# Libraries and variables
"""

#number_of_windows = 25000
window_size = 600
percent_testing=.05
percent_validation=.05
percent_training=.5
batch_size = 128
number_of_epochs=40
number_of_channels=18

#This is the period before a seizure that will be examined.
Period_of_interest=60*60*1

folders_for_making_model = ('chb06')

import os
import pandas as pd
import numpy as np

from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.python import training

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)
import random as r
import scipy as sp
from scipy.stats import zscore
from scipy.fft import fft, ifft

from keras.layers.rnn import LSTM

dfs=[]
#for folder in folders_for_making_model:
  #print(folder)
  #os.chdir(folder)
for x in os.listdir():
    if x.endswith("formated.csv"):
      print(x)
      dfs.append( pd.read_csv(str(x)))
#os.chdir("..")

time_between_signals = 3.906266
#This window size generates a number of samplings in a window of specific length
number_of_windows=(len(dfs)*625000/600)
window_time= (window_size*time_between_signals)/1000

validation_windows=int(number_of_windows*percent_validation)
testing_windows=   int(number_of_windows*percent_testing)
training_windows=  int(number_of_windows*percent_training)

print("the Period of interest is",Period_of_interest/3600,"hour\n",
      "This setup will generate:\n",training_windows , " windows for training,\n",
      validation_windows,"windows for validation &\n",
      testing_windows,"Windows for testing.\n",
      "Each window has",window_time,"seconds, and",window_size,"timesteps\n",
      "This is about",number_of_windows*window_time/60/60,"hours of data\n")

"""# Reading & Shaping Data"""

#for recording the lables of the dataset as a one-hot vector
def LabelVector3(label):
  zero_vector=[1,0,0]
  one_vector =[0,1,0]
  two_vector =[0,0,1]

  if label==0:
      return zero_vector
  elif label ==1:
      return one_vector
  else: return two_vector

def LabelVector2(label):
  zero_vector=[1,0]
  one_vector =[0,1]
  if label==0:
      return zero_vector
  else :
      return one_vector

#This ensures that no two training windows are the same.
#It generates a vector of shuffled unique start locations
def vectorMakerV2():
  arr=[]
  for df in range(0,len(dfs)):
    for row in range(0,len(dfs[df])-window_size-1,window_size):
        arr.append([df,row])
  np.random.shuffle(arr)
  arr=np.array(arr)

  window_df_starts = list(arr[:,0])
  window_starts= list(arr[:,1])
  return window_starts,window_df_starts

def frameArrayMakerV3(i, window_length,  window_starts, window_df_starts,normalize,fft_flag, include_seizures):
  frames_array=[]
  label_array=[]
  label_OHV=[]

  while i<window_length:

    start=window_starts[i]
    df=window_df_starts[i]
    i=i+1
    #Still ordered data from a dataframe of a specific size
    new_df=(dfs[df]).loc[start:start+window_size-1]

  #Stores the labels in an array
    #If the number is negative, it is a seizure and is marked with a 2
    if (new_df.at[start+window_size-1,"Label"] <0):
      new_label=2
    #if the label is 0, it is after the last siezure, i don;t really know how to treat it
    elif (new_df.at[start+window_size-1,"Label"] ==0):
      new_label = 0
    #if the label is greater than the poi, then it is far from the seizure and is marked with a 0
    elif(new_df.at[start+window_size-1,"Label"] >Period_of_interest):
      new_label=0
    #If none of the others were true, it must be in the period of interest, and it gets marked with a 1
    else:
      new_label=1


    if ((include_seizures == True) or (new_label != 2) ):
      new_df=new_df.drop(columns=["EEG Time", "Label"])
      # normalizes the columns
      if normalize == True:
          new_df = zscore(new_df)
      label_array.append(new_label)

      if (include_seizures == True):
          label_OHV.append( LabelVector3(new_label))
      else:
          label_OHV.append( LabelVector2(new_label))

      if fft_flag == True:
        imaginary_array = []
        fft_array=[]
        for j in range(0,number_of_channels):
            one_channel=new_df[:,j]
            anfft=fft(one_channel)
            fft_array.append(anfft.real[0:(window_size//2)])
            imaginary_array.append(anfft.imag[0:(window_size//2)])
        new_df=fft_array.append(imaginary_array)
      frames_array.append( new_df)


  return frames_array, label_array, label_OHV,i

#This function guarantees that the number of each training label should stay aproximately even
def rebalanceRatio(training_ds_p, training_lables_p,training_labels_OHV_p):

  training_ds=training_ds_p[0:training_windows]
  training_lables=training_lables_p[0:training_windows]
  training_labels_OHV=training_labels_OHV_p[0:training_windows]

  ratio=training_lables.count(1)/training_windows
  print("ratio before adjusting:",ratio)

  #a moves through the tail
  #b moves through the training window

  a=training_windows
  b=0
  while ratio >.505 and(a<len(training_ds_p)-2)and(b<training_windows-1):
    while training_lables_p[a] !=0and (a<len(training_ds_p)-1):
      a+=1
    while training_lables[b] !=1 and (b<training_windows-1):
      b+=1
    training_lables[b]=training_lables_p[a]
    training_ds[b]=training_ds_p[a]
    training_labels_OHV[b]=training_labels_OHV_p[a]
    a+=1
    b+=1
    ratio=training_lables.count(1)/training_windows

  while ratio <.495 and(a<len(training_ds_p)-1)and(b<training_windows-1):
    while training_lables_p[a] !=1 and (a<len(training_ds_p)-1):
      a+=1
    while training_lables[b] !=0and(b<training_windows-1):
      b+=1
    training_lables[b]=training_lables_p[a]
    training_ds[b]=training_ds_p[a]
    training_labels_OHV[b]=training_labels_OHV_p[a]
    a+=1
    b+=1
    ratio=training_lables.count(1)/training_windows

  return training_ds,training_lables,training_labels_OHV

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
#If print ratio is true, it will print the number of each type of label and their ratio
#if normalize is true, it will perform z-score normalization
def get_dataset_partitions_tf(dfs, hot_vector=True, print_ratio = True, normalize=False, fft=False, include_seizures = True):

    window_starts,window_df_starts= vectorMakerV2()

    #For testing data
    testing_ds, testing_lables,testing_labels_OHV,i =frameArrayMakerV3(0,testing_windows+1,  window_starts,window_df_starts,normalize,fft,include_seizures)

    #For validation data
    validation_ds, validation_lables,validatiion_labels_OHV,i =frameArrayMakerV3(i,validation_windows+testing_windows+1,  window_starts,window_df_starts,normalize,fft,include_seizures)

     #for training data
    training_ds_p, training_lables_p,training_labels_OHV_p,i =frameArrayMakerV3(i,len(window_starts), window_starts,window_df_starts,normalize,fft,include_seizures)
    #print(i,window_starts[30],window_df_starts[30])
    print('\n---Training Data---')

    training_ds, training_lables,training_labels_OHV = rebalanceRatio(training_ds_p, training_lables_p,training_labels_OHV_p)
    #0: not poi or seizure
    #1: period of interest
    #2: seizure



    if print_ratio == True:
      print("# of POI     :",training_lables.count(1),",",100*training_lables.count(1)/training_windows,'%')
      print("# of not POI :",training_lables.count(0),",",100*training_lables.count(0)/training_windows,'%')
      print("# of seizures:",training_lables.count(2),",",100*training_lables.count(2)/training_windows,'%','\n')
    if hot_vector == True:
      return training_ds, testing_ds, validation_ds, training_labels_OHV, testing_labels_OHV,validatiion_labels_OHV
    return training_ds, testing_ds, validation_ds, training_lables, testing_lables, validation_lables

#This generates the windows for training, testing, and validation.
#If hot vectors it true it will return your labels in the form of a a one-hot-vector
train_ds, test_ds, valid_ds, train_lables, test_lables,  valid_lables = get_dataset_partitions_tf(dfs,print_ratio=True, normalize=True, include_seizures=False, hot_vector=True)

training_data = np.array(train_ds)
testing_data = np.array(test_ds)
validation_data= np.array(valid_ds)


training_lables= np.array(train_lables)
testing_lables= np.array(test_lables)
validation_lables=np.array(valid_lables)


print("training  ",training_data.shape,",",training_lables.shape)
print("testing   ",testing_data.shape,",",testing_lables.shape)
print("validating",validation_data.shape,",",validation_lables.shape)
print('\n')

"""# Models & Training"""

learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=.01,
                                                                first_decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)
opt = keras.optimizers.legacy.Adam(learning_rate=learning_rate)

# normalized .5466,
input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(-1,window_size, number_of_channels, 1)

print("training  ",training_data.shape,",",training_lables.shape)
m2Dlstm = keras.Sequential([
  layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
  layers.Conv2D(filters=16, kernel_size=(3,3)),
  layers.MaxPool2D( pool_size=(11, 5)),
  layers.Reshape((54,32)),
  layers.LSTM(90, activation='relu' ),
  layers.Dense(75),
  layers.Dense(40),
  layers.Dense(training_lables.shape[1])
])
print('\n',m2Dlstm.summary())
m2Dlstm.compile(
              optimizer='Adam',
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])

m2Dlstm.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

"""# m1DLSTM

"""

input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(training_windows,-1, number_of_channels,1)

m1DLSTM = keras.Sequential([
  layers.Conv1D(filters=64, kernel_size=3, input_shape=input_shape),
  layers.Conv1D(filters=64, kernel_size=3),
  layers.MaxPool2D(),
  layers.Reshape([300,448]),
  layers.LSTM(100, activation='relu' ),
  layers.Dense(100),
  layers.Dense(50),
  layers.Dense(2)

])
print(m1DLSTM.summary())
m1DLSTM.compile(
              loss='categorical_crossentropy',
              metrics=['accuracy'])
m1DLSTM.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

input_shape = ( window_size, number_of_channels, 1)
training_data = training_data.reshape(training_windows,window_size, number_of_channels, 1)

m2Dconv = keras.Sequential([
  layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
  layers.Conv2D(filters=16, kernel_size=(3,3)),
  layers.MaxPool2D( pool_size=(50, 5)),
  layers.Flatten(),
  layers.Dense(100),
  layers.Dense(training_lables.shape[1])
])
print(m2Dconv.summary())
m2Dconv.compile(
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])
#normalized:.4955 , non-normalized: 0.4955
m2Dconv.fit(training_data, training_lables, batch_size=batch_size, epochs=number_of_epochs, validation_data=(validation_data, validation_lables))

print('\n--Accuracy on testing data--')
print('m2Dlstm')
test_loss, test_acc = m2Dlstm.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)

print('m1DLSTM')
test_loss, test_acc = m1DLSTM.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)

print('m2Dconv')
test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_lables, verbose=2)
print( test_acc)