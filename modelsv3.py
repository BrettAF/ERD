# -*- coding: utf-8 -*-
"""ModelsV3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbh0sgFnhcrcW02DCMiE4j-JAQ0ekBrt
"""

import tensorflow as tf
from tensorflow import keras
from keras import layers
from tensorflow.python import training
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import numpy as np
import time
import csv
import os

batch_size=200
number_of_epochs=15
window_size = 600
number_of_channels = 18

def make_dataset():
  os.chdir('/content/drive/MyDrive/ERD')
  data_labels=np.load('data_labels.npy')
  data_files=np.load('data_files.npy')
  data_labels=oneHot(data_labels)
  training_data,testing_data,training_labels,testing_labels=train_test_split(data_files,data_labels, test_size=0.33, random_state=42)

  return training_data,training_labels,testing_data,testing_labels

"""# MAIN"""

np_files=['chb01']#,'chb02','chb03','chb04','chb05','chb06','chb07','chb08','chb09','chb10','chb11','chb12','chb13','chb14','chb15','chb16','chb17','chb18','chb19','chb20', 'chb21','chb22','chb23','chb23','chb24']
l=250
training_data,training_labels,testing_data,testing_labels= make_dataset()

my_training_batch_generator = My_Custom_Generator(training_data, training_labels, batch_size)
my_testing_batch_generator = My_Custom_Generator(testing_data, testing_labels, batch_size)

#m2dlstm=fun2dlstm(training_data,training_labels,window_size)
#m1DLSTM=fun1DLSTM(training_data,training_labels,window_size)
m2Dconv=fun2Dconv(training_data,training_labels,window_size)
#print_report(m2dlstm,m1DLSTM,m2Dconv, testing_data,testing_labels)

test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_labels, verbose=2)
print( test_acc)

print(training_labels)

"""
# Models"""

def fun2dlstm(training_data,training_labels,window_size):
  number_of_channels=18

  outputs=2
  input_shape = ( window_size, number_of_channels, 1)

  print("training  ",training_data.shape,",",training_labels.shape)
  m2Dlstm = keras.Sequential([
      layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
      layers.Conv2D(filters=16, kernel_size=(3,3)),
      layers.MaxPool2D( pool_size=(11, 5)),
      layers.Reshape((54,32)),
      layers.LSTM(90, activation='relu' ),
      layers.Dense(75),
      layers.Dense(40),
      layers.Dense(outputs)
  ])
  print('\n',m2Dlstm.summary())
  m2Dlstm.compile(
                  optimizer='Adam',
                  loss=tf.keras.losses.CategoricalCrossentropy(),
                  metrics=['accuracy'])

  m2Dlstm.fit(generator=my_training_batch_generator,
                   steps_per_epoch = int(3800 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1)
  m2Dlstm.save("m2dlstm")
  return m2dlstm

def fun1DLSTM(training_data,training_labels,window_size):
  input_shape = ( window_size, number_of_channels, 1)


  m1DLSTM = keras.Sequential([
    layers.Conv1D(filters=64, kernel_size=3, input_shape=input_shape),
    layers.Conv1D(filters=64, kernel_size=3),
    layers.MaxPool2D(),
    layers.Reshape([300,448]),
    layers.LSTM(100, activation='relu' ),
    layers.Dense(100),
    layers.Dense(50),
    layers.Dense(outputs)

  ])
  print(m1DLSTM.summary())
  m1DLSTM.compile(
                optimizer='Adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
  m1DLSTM.fit(generator=my_training_batch_generator,
                   steps_per_epoch = int(3800 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1)
  m1DLSTM.save('m1DLSTM')
  return m1DLSTM

def fun2Dconv(training_data,training_labels,window_size):
  number_of_channels=18
  outputs=2

  input_shape = ( window_size, number_of_channels, 1)


  m2Dconv = keras.Sequential([
    layers.Conv2D(filters=32, kernel_size=(5,5) ,input_shape=input_shape),
    layers.Conv2D(filters=16, kernel_size=(3,3)),
    layers.MaxPool2D( pool_size=(50, 5)),
    layers.Flatten(),
    layers.Dense(100),
    layers.Dense(outputs)
  ])
  print(m2Dconv.summary())
  m2Dconv.compile(
                optimizer='Adam',
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=['accuracy'])


  m2Dconv.fit_generator(generator=my_training_batch_generator,
                   steps_per_epoch = int(3800 // batch_size),
                   epochs = number_of_epochs,
                   verbose = 1)
  m2Dconv.save("m2DLSTM")
  return m2Dconv

def print_report(m2dlstm,m1DLSTM,m2Dconv, testing_data,testing_labels):
  curr_time = str(time.strftime("%m-%d_%H:%M", time.localtime()))

  with open('results_'+curr_time+'.txt',"w") as writer:
    writer.write("batch size:"+str(batch_size)+"\tnumber of epochs:"+str(number_of_epochs))
    print('\n--Accuracy on testing data--')

    print('m2Dlstm')
    test_loss, test_acc = m2Dlstm.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write(str('m2Dlstm:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc)))

    print('m1DLSTM')
    test_loss, test_acc = m1DLSTM.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write('m1DLSTM:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc))

    print('m2Dconv')
    test_loss, test_acc = m2Dconv.evaluate(testing_data,testing_labels, verbose=2)
    print( test_acc)
    writer.write('m2Dconv:\t test loss:'+str(test_loss)+ ", test acc:"+str(test_acc))
  writer.close

test_loss, test_acc = m2Dlstm.evaluate(testing_data,testing_labels, verbose=2)
print( test_acc)

"""# Generator"""

learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=.01,
                                                                first_decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0, name=None)
opt = keras.optimizers.legacy.Adam(learning_rate=learning_rate)

class My_Custom_Generator(keras.utils.Sequence) :

  def __init__(self, filenames, labels, batch_size) :
    self.filenames = filenames
    self.labels = labels
    self.batch_size = batch_size


  def __len__(self) :
    return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)


  def __getitem__(self, idx) :
    batch_x = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]
    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]

    return np.array([np.memmap(str(file_name), dtype='float32', mode='w+', shape=(600,18))
               for file_name in batch_x]), np.array(batch_y)



#This function converts integers to one-hot arrays
def oneHot(labels):
  zero_vector=[1,0]
  one_vector =[0,1]
  new_array=[]
  for i in labels:
    if i==0:
        new_array.append(zero_vector)
    else :
        new_array.append(one_vector)
  new_array=np.array(new_array)
  return new_array